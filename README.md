# neuro_140 final project

- `load_data.py`: this contains methods for loading the Quick, Draw! data and example usage can be found throughout the notebooks below. The function `load_for_cnn` allows for the files stored in `object_files/` stored as `object_category_i.npz` to be loaded into train, test, and validation sets, with optional shuffling, binarization, standardization, reshaping for multiple channel axes, and upsampling. There is also a hand-made class provided for data augmentation `ImageDataGenerator`. This was ultimately not used because data augmentation was proven to harm more than it helps (e.g. a flipped canoe can become a moon), but it took an awfully long time to write.
- `Quick, Draw! Introduction.ipynb`: initial data processing, later ''factorized'' into `load_data.py`. Contains samples of images and labels that were instructive to examine
- `Transfer Learning on Quick, Draw!.ipynb`: contains the transfer learning tests using MobileNetV2 following the Tensorflow tutorial script. This was later ''factorized'' into a function in `VGG and Resnet Transfer Learning.ipynb`: self-explanatory. Contains a function that can be used with any model available in `tf.keras.applications`
- `Width and Depth Experiments on Quick, Draw!.ipynb`: this contains my optimization tests and visualization code for my hand-made CNNs. All of this code is my own with the exception of the activation visualization code (which was ultimately not included in this report), which was adapted from the code found at [this link](https://towardsdatascience.com/visualizing-intermediate-activation-in-convolutional-neural-networks-with-keras-260b36d60d0). 
- `Quick, Draw! CVAE with Visualization.ipynb`: this contains all of the code I used to train and test my CVAEs. The code for the CVAEs was adapted from the code found at [this link](https://www.machinecurve.com/index.php/2019/12/30/how-to-create-a-variational-autoencoder-with-keras/#), but the visualization and testing code is entirely my own. NB: for some bizzare reason, the code for training the CVAE only works with Tensorflow 1.12 while the code for visualizing the CVAE only works with Tensorflow 2.1. I don't know why this is the case, but swapping out kernels mid-notebook was an easy enough hack.
- `Load Images for Humans`: this was used to compare model performance to human performance. The labels were covered and the subjects (my parents) were made to guess the labels.
- `graveyard/`: this contains tutorials and preliminary pieces of code not used for the final report.
